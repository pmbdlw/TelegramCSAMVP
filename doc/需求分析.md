# Telegram 客服系统需求分析
## 功能需求分析

### Telegram服务相关

> 接口实现方面不算复杂。业务实现方面，重点架构策略考虑在实时消息通知方案设计，Telegram账号信息以及对应的联系人、对话信息的同步设计。此处架构设计最大的挑战部分是消息的实时通知吞吐量，响应时间和持久化，以及后续的消息检索性能，这部分会直接影响系统的可靠性和可用性。

1. Telegram账号登录/退出登录
2. 获取对应Telegram账号的对话列表
3. 获取对应Telegram账号对话的消息列表
4. Telegram消息实时通知
5. 回复Telegram消息/批量回复消息（涉及文件存储）
6. 其他

### 非Telegram 服务相关

> 1,2,3,4等属于常规业务功能，分析好之后不算复杂，对架构要求不算很高。重点架构策略放在消息检索，归档方面，大数据量和响应速度对架构设计要求较高。

1. 客服及上下级管理
   1. 管理员创建客服并设置客服类型（客服Leader和普通客服）
   2. 管理员及客服的登录流程
   3. 对应的数据流程就是对客服相关数据表的增删改查

2. TG账号分配
   1. TG账号的初始化，考虑是管理员批量导入，还是客服导入或者直接创建。此处的数据流也比较简单就是TG账号的信息记录
   2. TG账号分配。主要设置客服与账号的匹配关系。表结构设计要确认的是是否某个账号分配给某个客服后就只能由对应客服管理操作。这个问题会直接影响到客服与TG账号的表关系设计，也会影响到操作逻辑。

3. TG账号管理，分组，及打标签
   1. 账号打标签对应的标签表结构和账号与标签的对应关系相对简单。数据流也相对简单。TG账号给出的数量在300k低于百万级，主流关系数据库都不太会受影响。
   1. 分组类似于标签，这里也需要确认分组的目的是什么，好确认账号的分组和账号所属客服之间的关系。譬如客服1给几个TG账号分了组，客服组长收回了其中的一部分账号， 这时候分组是依旧存在还是怎么样？这里主要设计逻辑关系问题，会影响表关系设计和逻辑操作设计。

4. TG对话置顶（根据数据量要求我自己考虑有必要增加的功能）
   1. 由于一个客服管理多个TG账号，除了分组，标签检索相关TG账号外，某个阶段一定会有需要在某个时间节点持续关注的账号会话，这时候TG对话置顶或者某个TG账号置顶的功能就比较有用了。
   2. 正常列表展示的时候，参考各个即时通信软件或者客服对话软件的设计，都是以新消息的时间倒序排序，有最新消息进入的时候顺序排到前面。

5. Telegram消息打标签及检索（此处会产生性能需求）
   1. 根据给出的性能需求条件，SAAS的消息数据量会非常庞大，单纯的数据库检索肯定会遇到瓶颈
   2. 标签检索开始可以考虑使用关系存储标签与消息的对应关系，也考虑ElasticSearch等全文检索工具相关的实现（具体可行性需要调研）

6. Telegram消息存储及归档
   1. 这里Message的数据库设计的时候，要考虑设计冷热库和分表。冷热库逻辑比较好理解，按照每天平均产生1亿条消息的话，可以根据最后一条消息的时间对应的对话ID按月分冷热库，保存1个月内的活跃会话。另外考虑分出一个对话Achieve消息表保存所有已经结束的会话。


## 非功能需求分析

### 性能需求

1.  支持客服账号数量1k左右
    1.  客服账号数量主要影响系统操作的连接，主要考虑服务的相应能力就可以了，难度不是太大

2.  TG账号数量约 300k(低于百万级)
    1.  TG账号总数量低于百万级对性能影响也不是特别大

    2.  但是要考虑某些客服同时管理的TG账号数量过多的时候，实时通知服务与客服浏览器或者客户端的连接的稳定性（这部分需要调研一些资料确认设计）

    3.  TG账号总数量几十万级别的也会对与Telegram server的通信产生影响，这部分要考虑连接数，带宽等。另外需要调研Telegram Server是否对单节点的TG账号通信数量有限制。

3.  TG消息QPS（8k-12k），对实时性有要求，MQ选择依据
    1.  此处解释MQ的选择。我在澳新这边大家使用的主流MQ是Kafka和RabbitMQ，其他的我不熟悉，所以选型是从这两个里边进行的。

    2.  选择依据：Rab和Kaf的主要区别是低QPS的延迟和吞吐能力。作业给的性能要求是8k-12k区间，这个在Rabbit的延迟表现优秀的QPS范围内，几十k级别的QPS，Rabbit在延迟上有优势。不过我不确定这个QPS是最开始就这样，还是达到稳定业务量之后这样，如果后续QPS有需要提高到100k以上级别，就要考虑是最开始就直接选择高吞吐的Kafka，还是等QPS真的增长快的时候考虑更换。所以这里我暂时选择了RabbitMQ，另外在消息队列应用(主要是TG账号的信息，对话同步，以及消息的实时通知和持久化上)开发层面做好一个防腐层，如果不确定是否将来一定会有QPS的大量增长，就先使用RabbitMQ，等看到QPS增长趋势快的时候，在调整为Kafka。

4.  消息检索速度（根据QPS估算每天产生消息在亿级）。从数据层面做分库分表策略，冷热数据策略。
    1.  这部分上面功能需求分析时说明了从表设计方面如何保障检索速度。
    2.  如果有大范围检索要求（要看最终需求），要考虑使用全文检索工具来保障搜索的速度，因此这里考虑使用ElasticSearch


### 可靠及扩展需求

1. 从需求上看，数据量有肯能增长很快，因此最初就要考虑读写分离。应用程序编写开始就使用CQRS模式，将查询和操作分离开。
2. 对数据库和消息队列的使用要做好适配层，方便一旦需要切换数据库或者MQ产品或者根据需要拆分某些业务到其他更合适的数据库或者MQ产品
3. API服务，第一阶段考虑拆分Telegram 服务和常规管理服务。拆分依据是降低两种类型操作的耦合。譬如telegram server连接出现问题等，客服系统还可以继续进行非对话性操作。譬如归档历史消息等。
   1. Telegram服务用于直接和Telegram Server通信
   2. 非Telegram服务用于提供客服或者业务员进行的常规操作
4. 单独的消息通知服务（可以选用微软的SignalR，或者直接使用合适的websocket框架）用于Telegram消息的实时通知
   1. 需要配合MQ
   2. 大概逻辑是Telegram API注册的通知事件。每当Telegram有通知发生，针对通知类型发布相关的MQ事件。此处是有新消息，发布新消息事件。消息通知服务注册相关的事件消费逻辑，并在接收到消息通知时推送给客服。（新消息事件同时还有一个消费者是消息持久化，此消费者会把新消息记录到数据库中）
5. 此系统看似功能不多，但是对稳定性和扩展的运维要求还是比较高。因此容器编排工具也要从开始就要使用，做好发布策略